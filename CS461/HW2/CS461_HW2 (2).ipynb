{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "dWIJp4JTx1td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            #self.priors[i] = \n",
        "            pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        #log_probs = \n",
        "        \n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pkH1OvWXx0og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the multinomial Naive Bayes model, each document is represented as a bag of words and the number of occurrences of each word is used as a feature.\n",
        "\n",
        "Given a set of $m$ training documents, $C$ classes, and a vocabulary of $n$ words, let $x$ be a new document represented as a bag of words, where $x_i$ is the count of the i-th word in the vocabulary in the document. The goal is to find the class $y$ that maximizes the posterior probability, $P(y|x)$, using Bayes' Theorem:\n",
        "\n",
        ">$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$\n",
        "\n",
        "Here, $P(y)$ is the prior probability of the class, which can be estimated as the fraction of documents in the training set that belong to class y. $P(x|y)$ is the likelihood of the document given the class, which can be estimated as the product of the probabilities of each word in the vocabulary given the class. $P(x)$ is the normalizing constant, which is the same for all classes and can be ignored for the purposes of estimation.\n",
        "\n",
        "Using the multinomial distribution, the likelihood can be written as:\n",
        "\n",
        ">$P(x|y) = \\prod_{i=1}^n P(x_i|y)$\n",
        "\n",
        "Where $P(x_i|y)$ is the probability of observing word $i$ in a document given class $y$, which can be estimated as the count of word $i$ in class $y$ divided by the total count of all words in class $y$.\n",
        "\n",
        "Finally, taking the log of the posterior probabilities makes the calculation easier and allows us to find the MAP estimate by simply taking the maximum value:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log P(y|x) &= \\log \\frac{P(x|y)P(y)}{P(x)} \\\\\n",
        "&= \\log P(x|y) + \\log P(y) - \\log P(x) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "For prediction's we can ignore $\\log P(x)$ term and report the y that has highest $\\log P(y = i|x)$, where $i = {1,\\dots,c}$."
      ],
      "metadata": {
        "id": "cp5ohemuyLvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above implementation has two missing parts. Let's develop it step by step."
      ],
      "metadata": {
        "id": "skTv0AJO5HwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1: PART A [5 points]**\n",
        "\n",
        "Before writing any code, consider the following toy input. After fitting the dataset, what should be the value in `self.priors` ?\n",
        "\n",
        "Note: you need to just write down the answer."
      ],
      "metadata": {
        "id": "kS2NM7s_5l19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:\n",
        "\n"
      ],
      "metadata": {
        "id": "R668ansf6SOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "#here we assume n = 4, and each X, say [1,1,2,0] represents the corresponding counts of each of those words in that sentence.\n",
        "X = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 2, 1, 2], [1, 1, 0, 2]])\n",
        "# we have two classes \n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PIthJ5MMx-4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Q1: PART B [15 points]**\n",
        "\n",
        "In the `fit` function fill the missing line by uncommenting `self.priors[i] = ` and also remove `pass` after doing so. \n",
        "\n",
        "Note: Make edits in the below template.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vG_d4FoO6UUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            #self.priors[i] = \n",
        "            pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        #log_probs = \n",
        "        \n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n"
      ],
      "metadata": {
        "id": "g0z_u9VG6hoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(X, y)\n"
      ],
      "metadata": {
        "id": "FsdnL31MD7PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here fit calculates the p(y) and also stores two sets of parameters p(x|y) for y=0 and y=1. Let's check these before proceeding. (run the below cells)"
      ],
      "metadata": {
        "id": "1NX4IzXoyjls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb.priors"
      ],
      "metadata": {
        "id": "lYakGrx7yiZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb.counts.shape"
      ],
      "metadata": {
        "id": "zgKXy9J8y5mJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb.counts"
      ],
      "metadata": {
        "id": "MXt-esstzlBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1: PART C [15 points]**\n",
        "\n",
        "Complete the `log_probs = ` line in predict function. (code block is pasted below)\n",
        "\n",
        "Note: If you can't implement in a single line you are free to write it in multiple lines. If your implementation is correct below `assert` statement should run with no error."
      ],
      "metadata": {
        "id": "I61FIcvuyjDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            #self.priors[i] = \n",
        "            pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "      # returns a list of class labels for each x in X.\n",
        "      # Calculate the log probability of each class for each sample\n",
        "      #log_probs = \n",
        "        \n",
        "      # Return the class with the highest log probability for each sample\n",
        "      return self.classes[np.argmax(log_probs, axis=1)]\n"
      ],
      "metadata": {
        "id": "_5hLOnmB0yDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we assume n = 4, and each X, say [1,1,2,0] represents the corresponding counts of each of those words in that sentence.\n",
        "X = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 2, 1, 2], [1, 1, 0, 2]])\n",
        "# we have two classes \n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(X, y)\n",
        "\n",
        "assert np.all(nb.predict(X)==y),\"your predictions are wrong\""
      ],
      "metadata": {
        "id": "00SpKDb88Nei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test the effectivness of this algorithm on a real-world data set. "
      ],
      "metadata": {
        "id": "VDz_PuKexylM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "#look at the below cells and understand the dataset."
      ],
      "metadata": {
        "id": "QR48XYIoteNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data set has 20 different types of news"
      ],
      "metadata": {
        "id": "QEiXOOnk1grq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(newsgroups_train.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKf5h-6n-Paj",
        "outputId": "dcd572c2-f7fa-4d7c-ae64-db242cc890a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(newsgroups_train.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH8qCeWV1pw3",
        "outputId": "a7b9dab6-da0a-4d43-f6c9-073b7e1449ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's see a sample\n",
        "newsgroups_train.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "rMZHDfH608dF",
        "outputId": "207c1547-c88b-4259-a50e-bdac952ea192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The target attribute is the integer index of the category\n",
        "newsgroups_train.target[0],list(newsgroups_train.target_names)[newsgroups_train.target[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7KkXB0j1KSD",
        "outputId": "7e80cf6b-1349-4505-ed32-45e1dcdbf208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 'rec.autos')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2: PART A [15 POINTS]**\n",
        "\n",
        "Before proceeding, we need to convert the text into Bag of words format(as we saw in toy example). Sklearn has a built in function for it. Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for it below and fill the missing lines"
      ],
      "metadata": {
        "id": "e3Kb4O9z9vGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text data into a bag-of-words representation\n",
        "\n",
        "vectorizer = \n",
        "X_train = \n",
        "X_test = \n",
        "\n",
        "# Note both train and test data are newsgroups_train.data,newsgroups_test.data respectively. "
      ],
      "metadata": {
        "id": "aUAbRv3P-miY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2: PART B [15 POINTS]**\n",
        "\n",
        "If your implementation of all the above parts is correct, you should be able to run the below cell. Fill in the below cell for `accuracy = `and report your result ?"
      ],
      "metadata": {
        "id": "eU8MOp3cAW1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:"
      ],
      "metadata": {
        "id": "yYKp9Oz2AnzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the multinomial Naive Bayes model on the training data\n",
        "mnb = MultinomialNaiveBayes()\n",
        "mnb.fit(X_train, newsgroups_train.target)\n",
        "\n",
        "# Predict the class labels of the testing samples\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "\n",
        "#accuracy = \n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "HiAMATGcuQOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2: PART C [5 Points]**\n",
        "\n",
        "In your code for calculating the counts(`self.counts`) \n",
        "\n",
        "```\n",
        "self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "```\n",
        "\n",
        "```\n",
        " self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        " ```\n",
        "\n",
        " Why do we add 1 in the numerator and include n_features ? Give a short explanation ?\n",
        "\n",
        " Hint: It's called Laplace smoothing. Figure out why it's being used here."
      ],
      "metadata": {
        "id": "tVtPN5h72WR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:\n",
        "\n"
      ],
      "metadata": {
        "id": "kv_Fgswz_3UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-m5tItVhZfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider this data set\n",
        "\n",
        "| Feature 1 | Feature 2 | Class |\n",
        "|-----------|-----------|-------|\n",
        "| 1         | 1         | 0     |\n",
        "| 2.2         | 1.6         | 0     |\n",
        "| 2.5         | 1.8         | 0     |\n",
        "| 2.8         | 1.5         | 0     |\n",
        "| 2.9         | 1.2         | 0     |\n",
        "| 3.0        | 3.0        | 1     |\n"
      ],
      "metadata": {
        "id": "LuR9P1Wk6PgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the above data and train a logistic regression model on this data.\n",
        "\n",
        "Here are the learned weights: \n",
        "> $w_0$=  -4.08673095\n",
        "\n",
        "> $w_1$= 0.33123783\n",
        "\n",
        "> $w_2$= 0.89782125\n",
        "\n",
        ">Here $w_0$ corresponds to the intercept.\n",
        "\n",
        "</br>\n",
        "\n",
        "Here are the equations we discussed for class-conditional probabilities on a data-point $x_i = [x_{i1}, x_{i2}, ..., x_{ip}]$ :\n",
        "\n",
        ">$ P(y=1|\\mathbf{x_i}) = \\frac{1}{1 + e^{z}} $ \n",
        "\n",
        ">$ P(y=0|\\mathbf{x_i}) = \\frac{e^{z}}{1 + e^{z}} $ \n",
        "\n",
        ">Where $z = w_0 + w_1 x_{i1} + w_2 x_{i2} + ... + w_p x_{ip}$\n",
        "\n",
        ">Here $w_0$ is the intercept, $w_1, w_2, ..., w_p$ are the coefficients of the predictor variables $x_{i1}, x_{i2}, ..., x_{ip}$ for the $i$-th training sample, respectively.\n",
        "\n",
        "**Q3: PART A [10 points]**\n",
        "\n",
        "What is the accuracy over training data? (show the steps)\n",
        "\n",
        "**Q3: PART B [10 points]**\n",
        "\n",
        "Is the decision boundary linear or non-linear (5 points)? \n",
        "\n",
        "Please write down the expression for the decision boundary (5 points).\n",
        "\n",
        "**Q3: PART C [10 points]**\n",
        "\n",
        "List all the classifiers we have learned, that are generative classifiers. What are their properties? (5 points)\n",
        "\n",
        "List all the classifiers we have learned that are discriminative classifiers. What are their properties? (5 points)"
      ],
      "metadata": {
        "id": "_ZHPvPOo5oO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:\n",
        "\n"
      ],
      "metadata": {
        "id": "5T_9mk_U_I8m"
      }
    }
  ]
}